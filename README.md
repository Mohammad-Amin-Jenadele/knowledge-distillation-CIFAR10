# knowledge-distillation-CIFAR10
## Main Idea
Knowledge distillation is model compression method in which a small model is trained to mimic a pre-trained, larger model (or ensemble of models). This training setting is sometimes referred to as "teacher-student", where the large model is the teacher and the small model is the student (we'll be using these terms interchangeably).
The method was first proposed by Bucila et al., 2006 and generalized by [Hinton et al., 2015](https://arxiv.org/abs/1503.02531). The implementation in Distiller is based on the latter publication. Here we'll provide a summary of the method. For more information the reader may refer to the paper.

In distillation, knowledge is transferred from the teacher model to the student by minimizing a loss function in which the target is the distribution of class probabilities predicted by the teacher model. That is - the output of a softmax function on the teacher model's logits. However, in many cases, this probability distribution has the correct class at a very high probability, with all other class probabilities very close to 0. As such, it doesn't provide much information beyond the ground truth labels already provided in the dataset. To tackle this issue, [Hinton et al., 2015](https://arxiv.org/abs/1503.02531) introduced the concept of "softmax temperature".
The probability $p_i$ of class $i$ is calculated from logits $z$ as:
$$p_i = {{exp({z_i\over T})} \over {\sum_{j} exp({z_i\over T})}}$$
where $T$ is the temperature parameter. When $T=1$ we get the standard softmax function. As $T$ grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes the teacher found more similar to the predicted class. Hinton calls this the "dark knowledge" embedded in the teacher model, and it is this dark knowledge that we are transferring to the student model in the distillation process. When computing the loss function vs. the teacher's soft targets, we use the same value of $T$
 to compute the softmax on the student's logits. We call this loss the "distillation loss".
 [Hinton et al., 2015](https://arxiv.org/abs/1503.02531) found that it is also beneficial to train the distilled model to produce the correct labels (based on the ground truth) in addition to the teacher's soft-labels. Hence, we also calculate the "standard" loss between the student's predicted class probabilities and the ground-truth labels (also called "hard labels/targets"). We dub this loss the "student loss". When calculating the class probabilities for the student loss we use $T=1$.
The overall loss function, incorporating both distillation and student losses, is calculated as:
$$L(x;W) = \alpha * H(y,\sigma(z_s;T = 1)) + \beta * H(\sigma(z_t;T = \tau) , \sigma(z_s , T = \tau)) $$
where $x$ is the input , $W$ are the student model parameters , $y$ is the ground truth label , $H$ is the cross-entropy loss function , $\sigma$ is the softmax function parameterized by the temperature $T$, and $\alpha$ and $\beta$ are coefficients  $z_s$ and $z_t$ are the logits of the students and teacher repectively. For better understanding , see the picture below:

![image](https://github.com/amirhosein-ziaei/knowledge-distillation-CIFAR10/assets/167971975/62d7362f-6d4e-4437-9652-2dc8a623f5ed)

## New Hyper-Parameters
in general $\tau$ , $\alpha$ , $\beta$ are hyper parameters but [Hinton et al., 2015](https://arxiv.org/abs/1503.02531) used a weighted average between the distillation loss and the student loss which leads to : $$\beta = 1 - \alpha$$
In their experiments, [Hinton et al., 2015](https://arxiv.org/abs/1503.02531) use temperature values ranging from 1 to 20. They note that empirically, when the student model is very small compared to the teacher model, lower temperatures work better. This makes sense if we consider that as we raise the temperature, the resulting soft-labels distribution becomes richer in information, and a very small model might not be able to capture all of this information. However, there's no clear way to predict up front what kind of capacity for information the student model will have.\
[Source](https://intellabs.github.io/distiller/knowledge_distillation.html)
## Our Implementation
The purpose of this implementation is to show the power of knowledge distillation. First, <kbd>Resnet</kbd> Model is trained with initialized <kbd>Imagenet </kbd> weights on <kbd>Cifar10</kbd> dataset and the trained model is saved for knowledge distillation . Second, <kbd>MobileNetV2</kbd> Model is trained with random initialized weights on <kbd>Cifar10</kbd> dataset. Third, knowledge distillation is implemented. The student model is <kbd>MobileNetV2</kbd> and teacher model is <kbd>trained Resnet50 Model</kbd> which was trained in the first part. At last , all the three parts results are compared.

<kbd>utils.py</kbd> contains some frequenty used functions such as plotting confusion matrix , plotting some pictures and etc...

<kbd>distillation.py</kbd> contains distillation class in which knowledge distillation method is implemented.

<kbd>main.Ipynb</kbd> is the jupyter notebook file which contains of 3 parts:
1. Transfer learning using Resnet50 model on Cifar10 dataset:
   - In this part , <kbd>Resnet50</kbd> with <kbd>Imagenet</kbd> initialized weights is trained on <kbd>Cifar10</kbd> dataset.
2. Cifar10 Learning with MobileNetV2:
   - In this part , <kbd>MobileNetV2</kbd> model with random initialized weights is trained on <kbd>Cifar10</kbd> dataset.
3. Knowledge Distillation:
   - In this part knowledge distillation method is implemented. The <kbd>MobileNetV2</kbd> model with random initialized weights  plays as the <kbd>student</kbd> and pretrained <kbd>Resnet50</kbd> model plays as <kbd>teacher</kbd> 
